{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesión 15: Clasificador K-Nearest Neighbors (I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "El modelo para kNN es todo el conjunto de datos de entrenamiento. Cuando se requiere una predicción para una instancia de datos invisible, el algoritmo kNN buscará en el conjunto de datos de entrenamiento las instancias más similares. El atributo de predicción de las instancias más similares se resume y devuelve como la predicción para la instancia invisible.\n",
    "\n",
    "La medida de similitud depende del tipo de datos. Para datos de valor real, se puede usar la distancia euclidiana.\n",
    "\n",
    "## Como trabaja k-Nearest Neighbors\n",
    "\n",
    "El algoritmo kNN pertenece a la familia de algoritmos de aprendizaje competitivo y aprendizaje perezoso basados en instancias.\n",
    "\n",
    "Los algoritmos basados en instancias son aquellos algoritmos que modelan el problema utilizando instancias de datos (o filas) para tomar decisiones predictivas. El algoritmo kNN es una forma extrema de métodos basados en instancias porque todas las observaciones de entrenamiento se retienen como parte del modelo.\n",
    "\n",
    "Es un algoritmo de aprendizaje competitivo, porque internamente utiliza la competencia entre elementos del modelo (instancias de datos) para tomar una decisión predictiva. La medida objetiva de similitud entre instancias de datos hace que cada instancia de datos compita para \"ganar\" o sea más similar a una instancia de datos invisible y contribuya a una predicción.\n",
    "\n",
    "El aprendizaje diferido se refiere al hecho de que el algoritmo no construye un modelo hasta el momento en que se requiere una predicción. Es vago porque solo funciona en el último segundo. Esto tiene la ventaja de incluir solo datos relevantes para los datos invisibles, denominado modelo localizado. Una desventaja es que puede ser costoso desde el punto de vista informático repetir las mismas o similares búsquedas en conjuntos de datos de entrenamiento más grandes.\n",
    "\n",
    "Finalmente, kNN es poderoso porque no asume nada sobre los datos, aparte de que una medida de distancia puede calcularse de manera consistente entre dos instancias. Como tal, se llama no paramétrico o no lineal, ya que no asume una forma funcional.\n",
    "\n",
    "## Ejemplo usando la data iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the iris data into a DataFrame\n",
    "import pandas as pd\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "iris = pd.read_csv(url, header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow plots to appear in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# increase default figure and font sizes for easier viewing\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# create a custom colormap\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each iris species to a number\n",
    "iris['species_num'] = iris.species.map({'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of PETAL LENGTH versus PETAL WIDTH and color by SPECIES\n",
    "iris.plot(kind='scatter', x='petal_length', y='petal_width', c='species_num', colormap=cmap_bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of SEPAL LENGTH versus SEPAL WIDTH and color by SPECIES\n",
    "iris.plot(kind='scatter', x='sepal_length', y='sepal_width', c='species_num', colormap=cmap_bold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando un clasificador KNN\n",
    "\n",
    "### Estimando similaridad\n",
    "\n",
    "Para hacer predicciones, necesitamos calcular la similitud entre dos instancias de datos dadas. Esto es necesario para que podamos localizar las k instancias de datos más similares en el conjunto de datos de entrenamiento para un miembro dado del conjunto de datos de prueba y, a su vez, hacer una predicción.\n",
    "\n",
    "Dado que las cuatro medidas de flores son numéricas y tienen las mismas unidades, podemos usar directamente la medida de distancia euclidiana. Esto se define como la raíz cuadrada de la suma de las diferencias cuadráticas entre las dos matrices de números (léala varias veces y deje que se hunda).\n",
    "\n",
    "Además, queremos controlar qué campos incluir en el cálculo de la distancia. Específicamente, solo queremos incluir los primeros 4 atributos. Un enfoque es limitar la distancia euclidiana a una longitud fija, ignorando la dimensión final.\n",
    "\n",
    "Al unir todo esto, podemos definir la función `euclideanDistance` de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def euclideanDistance(instance1, instance2):\n",
    "    distance = (instance1 - instance2) ** 2\n",
    "    # Check if either instance1 or instance2 is a matrix\n",
    "    if distance.shape[0] == distance.size:\n",
    "        return distance.sum() ** 0.5\n",
    "    else:\n",
    "        return distance.sum(axis=1) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.array([2, 2, 1])\n",
    "data2 = np.array([4, 4, 1])\n",
    "\n",
    "distance = data1 - data2\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.array([2, 2])\n",
    "data2 = np.array([4, 4])\n",
    "distance = euclideanDistance(data1, data2)\n",
    "print('Distance: ' + repr(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(data1[0], data1[1])\n",
    "plt.scatter(data2[0], data2[1])\n",
    "plt.plot([data1[0], data2[0]], [data1[1], data2[1]], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrando Vecinos\n",
    "\n",
    "\n",
    "Ahora que tenemos una medida de similitud, podemos usarla para recopilar las k instancias más similares para una instancia invisible.\n",
    "\n",
    "Este es un proceso sencillo de calcular la distancia para todas las instancias y seleccionar un subconjunto con los valores de distancia más pequeños.\n",
    "\n",
    "A continuación se muestra la función `getNeighbours` que devuelve k vecinos más similares del conjunto de entrenamiento para una instancia de prueba dada (utilizando la función` euclideanDistance` ya definida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = np.array([[2, 2], [4, 4], [7, 7], [4, 1], [3, 4], [5, 2]])\n",
    "testInstance = np.array([5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = euclideanDistance(trainSet, testInstance)\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos cuáles son los puntos más cercanos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.argsort()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeighbors(trainSet, testInstance, k):\n",
    "    dist = euclideanDistance(trainSet, testInstance)\n",
    "    neighbors = dist.argsort()[:k]\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2\n",
    "neighbors = getNeighbors(trainSet, testInstance, k)\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(trainSet[:, 0], trainSet[:, 1], s=50)\n",
    "plt.scatter(testInstance[0], testInstance[1], c='green', s=100)\n",
    "plt.plot([testInstance[0], trainSet[1, 0]], [testInstance[1], trainSet[1, 1]], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testInstance = np.array([3.4, 3])\n",
    "k = 3\n",
    "neighbors = getNeighbors(trainSet, testInstance, k)\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(trainSet[:, 0], trainSet[:, 1], s=50)\n",
    "plt.scatter(testInstance[0], testInstance[1], c='green', s=100)\n",
    "for neighbor in neighbors:\n",
    "    plt.plot([testInstance[0], trainSet[neighbor, 0]], [testInstance[1], trainSet[neighbor, 1]], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuesta\n",
    "\n",
    "Una vez que hemos localizado los vecinos más similares para una instancia de prueba, la siguiente tarea es diseñar una respuesta pronosticada basada en esos vecinos.\n",
    "\n",
    "Podemos hacer esto permitiendo que cada vecino vote por su atributo de clase y tome el voto mayoritario como la predicción.\n",
    "\n",
    "Primero definamos la etiqueta de cada instancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet_y = np.array([0, 0, 1, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(trainSet[trainSet_y==0, 0], trainSet[trainSet_y==0, 1], s=50)\n",
    "plt.scatter(trainSet[trainSet_y==1, 0], trainSet[trainSet_y==1, 1], c='y', s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se proporciona una función para obtener la respuesta votada por la mayoría de varios vecinos. Se supone que la clase es el último atributo para cada vecino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(trainSet[trainSet_y==0, 0], trainSet[trainSet_y==0, 1], s=50)\n",
    "plt.scatter(trainSet[trainSet_y==1, 0], trainSet[trainSet_y==1, 1], c='y', s=50)\n",
    "plt.scatter(testInstance[0], testInstance[1], c='green', s=100)\n",
    "for neighbor in neighbors:\n",
    "    plt.plot([testInstance[0], trainSet[neighbor, 0]], [testInstance[1], trainSet[neighbor, 1]], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet_y[neighbors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import itemfreq\n",
    "freq = itemfreq(trainSet_y[neighbors])\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq2 = pd.Series(trainSet_y[neighbors])\n",
    "freq2.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq[:, 0][freq[:, 1].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq[:, 1] / freq[:, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack((freq[:, 0], freq[:, 1] / freq[:, 1].sum())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(trainSet_y, neighbors):\n",
    "    votes = trainSet_y[neighbors]\n",
    "    freq = itemfreq(votes)\n",
    "    return freq[:, 0][freq[:, 1].argmax()], np.vstack((freq[:, 0], freq[:, 1] / freq[:, 1].sum())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = getResponse(trainSet_y, neighbors)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "Lets put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier_one(trainSet, trainSet_y, testInstance, k):\n",
    "    neighbors = getNeighbors(trainSet, testInstance, k)\n",
    "    pred_y, pred_prob = getResponse(trainSet_y, neighbors)\n",
    "    return pred_y, pred_prob, neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testInstance = np.array([4.2, 4.1])\n",
    "plt.scatter(trainSet[trainSet_y==0, 0], trainSet[trainSet_y==0, 1], s=50)\n",
    "plt.scatter(trainSet[trainSet_y==1, 0], trainSet[trainSet_y==1, 1], c='y', s=50)\n",
    "plt.scatter(testInstance[0], testInstance[1], c='green', s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 6):\n",
    "    print('k = ', k)\n",
    "    pred_y, pred_prob, neighbors = knn_classifier_one(trainSet, trainSet_y, testInstance, k)\n",
    "    print('pred_y = ', pred_y)\n",
    "    print('pred_prob = ', pred_prob)\n",
    "    plt.scatter(trainSet[trainSet_y==0, 0], trainSet[trainSet_y==0, 1], s=50)\n",
    "    plt.scatter(trainSet[trainSet_y==1, 0], trainSet[trainSet_y==1, 1], c='y', s=50)\n",
    "    plt.scatter(testInstance[0], testInstance[1], c='green', s=100)\n",
    "    for neighbor in neighbors:\n",
    "        plt.plot([testInstance[0], trainSet[neighbor, 0]], [testInstance[1], trainSet[neighbor, 1]], '--r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permitir más de una instancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testInstances = np.array([[4.2, 4.1], [1, 3], [6, 6]])\n",
    "plt.scatter(trainSet[trainSet_y==0, 0], trainSet[trainSet_y==0, 1], s=50)\n",
    "plt.scatter(trainSet[trainSet_y==1, 0], trainSet[trainSet_y==1, 1], c='y', s=50)\n",
    "plt.scatter(testInstances[:,0], testInstances[:,1], c='green', s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(trainSet, trainSet_y, testInstances, k):\n",
    "    n_samples_test = testInstances.shape[0]\n",
    "    pred_y = np.zeros(n_samples_test)\n",
    "    y_unique = np.unique(trainSet_y)\n",
    "    pred_prob = np.zeros((n_samples_test, y_unique.shape[0]))\n",
    "    for i in range(n_samples_test):\n",
    "        neighbors = getNeighbors(trainSet, testInstances[i], k)\n",
    "        pred_y_, pred_prob_ = getResponse(trainSet_y, neighbors)\n",
    "        pred_y[i] = pred_y_\n",
    "        \n",
    "        # pred_y may not include all values of y\n",
    "        for j in range(y_unique.shape[0]):\n",
    "            pred_prob[i, j] =  pred_prob_[pred_prob_[:,0] == y_unique[j], 1].sum()\n",
    "            \n",
    "    return pred_y, pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "knn_classifier(trainSet, trainSet_y, testInstances, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando a la data Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris.species_num\n",
    "X = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_pred_prob = knn_classifier(X_train, y_train, X_test, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "y_pred_prob = knn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencia: https://www.coursera.org/learn/python-machine-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
